{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "125d8168-960d-4c55-acf2-7b17308e5d8d",
   "metadata": {},
   "source": [
    "# **<font color=\"red\">Context</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "198b8d0f-8edc-496f-b658-a46437c8a2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from config import config\n",
    "os.environ[\"GOOGLE_API_KEY\"] = config.GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b68e0438-178f-483f-b3ef-9b1014c93953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: weather in london?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "D:\\Agent-Development-Kit\\venv\\Lib\\site-packages\\google\\adk\\flows\\llm_flows\\base_llm_flow.py:449: UserWarning: [EXPERIMENTAL] feature FeatureName.PROGRESSIVE_SSE_STREAMING is enabled.\n",
      "  async for event in agen:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Response: The current weather in London is cloudy with a temperature of 18 degrees Celsius and a chance of rain.\n"
     ]
    }
   ],
   "source": [
    "from google.adk.agents import Agent\n",
    "from google.adk.tools import FunctionTool\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.genai import types\n",
    "\n",
    "APP_NAME=\"weather_sentiment_agent\"\n",
    "USER_ID=\"user1234\"\n",
    "SESSION_ID=\"1234\"\n",
    "MODEL_ID=\"gemini-2.5-flash\"\n",
    "\n",
    "# Tool 1\n",
    "def get_weather_report(city: str) -> dict:\n",
    "    \"\"\"Retrieves the current weather report for a specified city.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the weather information with a 'status' key ('success' or 'error') and a 'report' key with the weather details if successful, or an 'error_message' if an error occurred.\n",
    "    \"\"\"\n",
    "    if city.lower() == \"london\":\n",
    "        return {\"status\": \"success\", \"report\": \"The current weather in London is cloudy with a temperature of 18 degrees Celsius and a chance of rain.\"}\n",
    "    elif city.lower() == \"paris\":\n",
    "        return {\"status\": \"success\", \"report\": \"The weather in Paris is sunny with a temperature of 25 degrees Celsius.\"}\n",
    "    else:\n",
    "        return {\"status\": \"error\", \"error_message\": f\"Weather information for '{city}' is not available.\"}\n",
    "\n",
    "weather_tool = FunctionTool(func=get_weather_report)\n",
    "\n",
    "\n",
    "# Tool 2\n",
    "def analyze_sentiment(text: str) -> dict:\n",
    "    \"\"\"Analyzes the sentiment of the given text.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with 'sentiment' ('positive', 'negative', or 'neutral') and a 'confidence' score.\n",
    "    \"\"\"\n",
    "    if \"good\" in text.lower() or \"sunny\" in text.lower():\n",
    "        return {\"sentiment\": \"positive\", \"confidence\": 0.8}\n",
    "    elif \"rain\" in text.lower() or \"bad\" in text.lower():\n",
    "        return {\"sentiment\": \"negative\", \"confidence\": 0.7}\n",
    "    else:\n",
    "        return {\"sentiment\": \"neutral\", \"confidence\": 0.6}\n",
    "\n",
    "sentiment_tool = FunctionTool(func=analyze_sentiment)\n",
    "\n",
    "\n",
    "# Agent\n",
    "weather_sentiment_agent = Agent(\n",
    "    model=MODEL_ID,\n",
    "    name='weather_sentiment_agent',\n",
    "    instruction=\"\"\"You are a helpful assistant that provides weather information and analyzes the sentiment of user feedback.\n",
    "**If the user asks about the weather in a specific city, use the 'get_weather_report' tool to retrieve the weather details.**\n",
    "**If the 'get_weather_report' tool returns a 'success' status, provide the weather report to the user.**\n",
    "**If the 'get_weather_report' tool returns an 'error' status, inform the user that the weather information for the specified city is not available and ask if they have another city in mind.**\n",
    "**After providing a weather report, if the user gives feedback on the weather (e.g., 'That's good' or 'I don't like rain'), use the 'analyze_sentiment' tool to understand their sentiment.** Then, briefly acknowledge their sentiment.\n",
    "You can handle these tasks sequentially if needed.\"\"\",\n",
    "    tools=[weather_tool, sentiment_tool]\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"Main function to run the agent asynchronously.\"\"\"\n",
    "# Session and Runner Setup\n",
    "session_service = InMemorySessionService()\n",
    "# Use 'await' to correctly create the session\n",
    "await session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
    "\n",
    "runner = Runner(agent=weather_sentiment_agent, app_name=APP_NAME, session_service=session_service)\n",
    "\n",
    "# Agent Interaction\n",
    "query = \"weather in london?\"\n",
    "print(f\"User Query: {query}\")\n",
    "content = types.Content(role='user', parts=[types.Part(text=query)])\n",
    "\n",
    "# The runner's run method handles the async loop internally\n",
    "events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
    "\n",
    "for event in events:\n",
    "    if event.is_final_response():\n",
    "        final_response = event.content.parts[0].text\n",
    "        print(\"Agent Response:\", final_response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44607e70-9400-4350-b4a0-b1ebc603efc0",
   "metadata": {},
   "source": [
    "## **Context:** \n",
    "- In ADK context refers to the crucial bundle of information available to your agent and its tools during specific operations.\n",
    "- Agents often need more that just the latest user message to perform well. Context is essential because it enables:\n",
    "  1. **Maintaining State:** Remember details across multiple steps in a conversation. This is primarily managed through _session state_.\n",
    "  2. **Passing Data:** Shared information discoverd or generated in one step(like an LLM call or a tool execution) with subsequent steps. Session State is key here too.\n",
    "  3. **Accessing Services:** Interacting with framewok capabilities like:\n",
    "     - **Artifact Storage:** Saving or loading files or data blobs (like PDFs, images, configuration files) associated with the session.\n",
    "     - **Memory:** Searching for relevant information from past interactions or external knowledge sources conected to the user.\n",
    "     - **Authentication:** Requesting and retrieving credentials needed by tools to access external APIs security.\n",
    "  4. **Identity and Tracking:** Knowing which agent is currently running (`agent.name`) and uniquely identifying the current request-response cycle (`invocation_id`) for logging and debugging.\n",
    "  5. **Tool-Specific Actions:** Enabling specialized operations withing tools, such as requesting authenticaiton or searching memory, which require access to the current interaction's details.\n",
    "- The central piece holding all this information together for a single, complete user-request-to-final-response cycle (an **invocation**) is the `InvocationContext`. However, you typically won't create or manage this object directly. The ADK framework creates it when an invocation starts (e.g., via `runner.run_async`) and passes the relevant contextual information implicitly to your agent code, callbacks, and tools.\n",
    "\n",
    "### **Types of Context:**\n",
    "1. `InvocationContext`\n",
    "2. `ReadonlyContext`\n",
    "3. `CallbackContext`\n",
    "4. `ToolContext`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368094a6-e949-4c20-b5c4-0f8cc24a54b2",
   "metadata": {},
   "source": [
    "### **<font color=\"blue\">InvocationContext</font>**\n",
    "- **Where Used:** Received as the ctx argument directly within an agent's core implementation methods (`_run_async_impl`, `_run_live_impl`).\n",
    "- **Purpose:** Provides access to the entire state of the current invocation. This is the most comprehensive context object.\n",
    "- **Key Contents:** Direct access to `session` (including `state` and `events`), the current `agent` instance, `invocation_id`, initial `user_content`, references to configured services (`artifact_service`, `memory_service`, `session_service`), and fields related to live/streaming modes.\n",
    "**Use Case:** Primarily used when the agent's core logic needs direct access to the overall session or services, though often state and artifact interactions are delegated to callbacks/tools which use their own contexts. Also used to control the invocation itself (e.g., setting `ctx.end_invocation = True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b8945d-2173-4db5-89b6-e8546d2bc266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode: Agent implementation receiving InvocationContext\n",
    "from google.adk.agents import BaseAgent\n",
    "from google.adk.agents.invocation_context import InvocationContext\n",
    "from google.adk.events import Event\n",
    "from typing import AsyncGenerator\n",
    "\n",
    "class MyAgent(BaseAgent):\n",
    "    async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:\n",
    "        # Direct access example\n",
    "        agent_name = ctx.agent.name\n",
    "        session_id = ctx.session.id\n",
    "        print(f\"Agent {agent_name} running in session {session_id} for invocation {ctx.invocation_id}\")\n",
    "        # ... agent logic using ctx ...\n",
    "        yield # ... event ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fbe55eb-f8f3-4404-b250-6b836d85f4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sending user message...\n",
      "\n",
      "============================================================\n",
      "Agent Name     : ContextAwareAgent\n",
      "Session ID     : session_1\n",
      "User ID        : user_1\n",
      "Invocation ID  : e-b320a9e5-27c6-488b-a253-7c7afa3b4923\n",
      "============================================================\n",
      "\n",
      "Agent Response:\n",
      "Hello user_1!\n",
      "This was processed in invocation: e-b320a9e5-27c6-488b-a253-7c7afa3b4923\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from typing import AsyncGenerator\n",
    "\n",
    "from google.adk.agents import BaseAgent\n",
    "from google.adk.agents.invocation_context import InvocationContext\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.events import Event\n",
    "from google.genai import types\n",
    "\n",
    "from config import config\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Configuration\n",
    "# -------------------------------------------------\n",
    "os.environ[\"GOOGLE_API_KEY\"] = config.GOOGLE_API_KEY\n",
    "\n",
    "APP_NAME = \"invocation_context_app\"\n",
    "USER_ID = \"user_1\"\n",
    "SESSION_ID = \"session_1\"\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Custom Agent Using InvocationContext\n",
    "# -------------------------------------------------\n",
    "class MyAgent(BaseAgent):\n",
    "\n",
    "    async def _run_async_impl(\n",
    "        self, ctx: InvocationContext\n",
    "    ) -> AsyncGenerator[Event, None]:\n",
    "\n",
    "        # Access InvocationContext data\n",
    "        agent_name = ctx.agent.name\n",
    "        session_id = ctx.session.id\n",
    "        invocation_id = ctx.invocation_id\n",
    "        user_id = ctx.session.user_id\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Agent Name     : {agent_name}\")\n",
    "        print(f\"Session ID     : {session_id}\")\n",
    "        print(f\"User ID        : {user_id}\")\n",
    "        print(f\"Invocation ID  : {invocation_id}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Create response\n",
    "        response_text = (\n",
    "            f\"Hello {user_id}!\\n\"\n",
    "            f\"This was processed in invocation: {invocation_id}\"\n",
    "        )\n",
    "\n",
    "        # Yield response event\n",
    "        yield Event(\n",
    "            author=self.name,\n",
    "            content=types.Content(\n",
    "                role=\"model\",\n",
    "                parts=[types.Part(text=response_text)],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Main Runner Logic\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Create session service\n",
    "session_service = InMemorySessionService()\n",
    "\n",
    "# Create session\n",
    "await session_service.create_session(\n",
    "    app_name=APP_NAME,\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_ID,\n",
    ")\n",
    "\n",
    "# Create agent\n",
    "agent = MyAgent(name=\"ContextAwareAgent\")\n",
    "\n",
    "# Create runner\n",
    "runner = Runner(\n",
    "    agent=agent,\n",
    "    app_name=APP_NAME,\n",
    "    session_service=session_service,\n",
    ")\n",
    "\n",
    "# Send a user message\n",
    "print(\"\\nSending user message...\\n\")\n",
    "\n",
    "async for event in runner.run_async(\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_ID,\n",
    "    new_message=types.Content(\n",
    "        role=\"user\",\n",
    "        parts=[types.Part(text=\"Hello Agent!\")],\n",
    "    ),\n",
    "):\n",
    "    if event.content and event.content.parts:\n",
    "        print(\"\\nAgent Response:\")\n",
    "        print(event.content.parts[0].text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b9c9e3-0e2d-43a6-b9d1-70b48b6cd762",
   "metadata": {},
   "source": [
    "### **<font color=\"blue\">ReadonlyContext</font>**\n",
    "- **Where Used:** Provided in scenarios where only read access to basic information is needed and mutation is disallowed (e.g., `InstructionProvider` functions). It's also the base class for other contexts.\n",
    "- **Purpose:** Offers a safe, read-only view of fundamental contextual details.\n",
    "- **Key Contents:** `invocation_id`, `agent_name`, and a read-only view of the current `state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ac7ee5e-67f1-4d4f-9612-4c2dc9af1269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode: Instruction provider receiving ReadonlyContext\n",
    "from google.adk.agents.readonly_context import ReadonlyContext\n",
    "\n",
    "def my_instruction_provider(context: ReadonlyContext) -> str:\n",
    "    # Read-only access example\n",
    "    user_tier = context.state().get(\"user_tier\", \"standard\") # Can read state\n",
    "    # context.state['new_key'] = 'value' # This would typically cause an error or be ineffective\n",
    "    return f\"Process the request for a {user_tier} user.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fcffe8d-ea0b-475b-953e-bf1d6240793f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sending message...\n",
      "\n",
      "Agent Response:\n",
      "\n",
      "As a premium user, you'll receive a detailed and comprehensive explanation of AI agents.\n",
      "\n",
      "---\n",
      "\n",
      "### What are AI Agents?\n",
      "\n",
      "At its core, an **AI agent** is anything that can perceive its environment through sensors and act upon that environment through effectors. It's a system designed to operate autonomously, pursuing specific goals, and often interacting with other agents or humans.\n",
      "\n",
      "Think of an AI agent as an intelligent entity that observes, thinks, and acts within a particular context. This definition is quite broad, encompassing everything from a simple thermostat to a complex self-driving car or a sophisticated large language model.\n",
      "\n",
      "### Core Components and Characteristics:\n",
      "\n",
      "To better understand AI agents, let's break down their fundamental components and characteristics:\n",
      "\n",
      "1.  **Environment:** This is the world in which the agent exists and operates. It can be physical (e.g., a room for a robot) or digital (e.g., the internet for a web crawler, a game world for a character, or a database for an information retrieval system).\n",
      "\n",
      "2.  **Sensors:** These are the \"eyes and ears\" of the agent. They allow the agent to gather information about its environment.\n",
      "    *   **Examples:** Cameras, microphones, temperature sensors, GPS, keyboards, API calls, data streams, text inputs, user prompts.\n",
      "\n",
      "3.  **Effectors:** These are the \"hands and voice\" of the agent. They enable the agent to perform actions that modify or interact with its environment.\n",
      "    *   **Examples:** Robotic arms, wheels, displays, speakers, motor controls, writing code, sending emails, generating text, making API calls, updating databases.\n",
      "\n",
      "4.  **Agent Function (or Architecture):** This is the \"brain\" of the agent. It maps percepts (what the agent senses) to actions (what the agent does). This function dictates the agent's behavior and decision-making logic. It's where the intelligence resides.\n",
      "\n",
      "5.  **Percept Sequence:** This is the complete history of everything the agent has ever perceived.\n",
      "\n",
      "### The Agent Cycle (Perceive-Reason-Act):\n",
      "\n",
      "AI agents typically operate in a continuous loop:\n",
      "\n",
      "1.  **Perceive:** The agent gathers information from its environment using its sensors.\n",
      "2.  **Reason/Decide:** Based on its current percepts, its internal state (knowledge, goals, memory), and its agent function, it decides what action to take next. This involves processing information, planning, learning, and problem-solving.\n",
      "3.  **Act:** The agent executes the chosen action through its effectors, influencing its environment.\n",
      "\n",
      "This cycle then repeats, allowing the agent to continuously adapt and respond to changes in its surroundings.\n",
      "\n",
      "### Key Attributes of Intelligent Agents:\n",
      "\n",
      "*   **Autonomy:** Agents can operate without constant human intervention, making decisions and initiating actions on their own.\n",
      "*   **Goal-Directedness:** Agents are designed to achieve specific objectives or optimize certain outcomes.\n",
      "*   **Adaptivity/Learning:** Many agents can learn from experience, improve their performance over time, and adapt to new situations or changing environments.\n",
      "*   **Reactivity:** Agents can respond to real-time changes or events in their environment.\n",
      "*   **Pro-activeness:** Agents can take initiative and pursue goals even without direct prompts or external stimuli.\n",
      "*   **Social Ability (for some agents):** Agents can interact and communicate with other agents or humans.\n",
      "\n",
      "### Types of AI Agents (Based on their Agent Function):\n",
      "\n",
      "AI agents can be categorized based on the complexity of their internal logic and how they map percepts to actions:\n",
      "\n",
      "1.  **Simple Reflex Agents:**\n",
      "    *   **Logic:** React directly to current percepts without considering past history or future consequences. They use \"condition-action rules.\"\n",
      "    *   **Example:** A thermostat that turns on the heater when the temperature drops below a certain threshold.\n",
      "\n",
      "2.  **Model-Based Reflex Agents:**\n",
      "    *   **Logic:** Maintain an internal state (a \"model\" of the world) based on their percept history. This allows them to handle partially observable environments and understand how their actions affect the world.\n",
      "    *   **Example:** A self-driving car that tracks its own position, speed, and the position of other cars, even if they momentarily disappear from sensor view.\n",
      "\n",
      "3.  **Goal-Based Agents:**\n",
      "    *   **Logic:** Not only maintain an internal model but also have explicit goals. They choose actions that will lead them towards their goals, often involving planning and search algorithms.\n",
      "    *   **Example:** A robot navigating a maze to reach a specific exit, planning its path based on its map and desired destination.\n",
      "\n",
      "4.  **Utility-Based Agents:**\n",
      "    *   **Logic:** Similar to goal-based agents, but they also have a \"utility function\" that measures the desirability of different states or outcomes. They choose actions that maximize their expected utility, especially in situations with uncertainty or conflicting goals.\n",
      "    *   **Example:** An automated stock trading agent that seeks to maximize profit while minimizing risk, evaluating various trade options based on their potential return and volatility.\n",
      "\n",
      "5.  **Learning Agents:**\n",
      "    *   **Logic:** All the above agent types can incorporate a learning element that allows them to improve their performance over time by analyzing feedback from their actions.\n",
      "    *   **Components:** Learning element (makes improvements), performance element (chooses actions), critic (provides feedback), problem generator (suggests experiments).\n",
      "    *   **Example:** A chess-playing AI that learns from playing against itself or human opponents, improving its strategies over time. A large language model learning from vast datasets and refining its responses based on user feedback.\n",
      "\n",
      "### Why are AI Agents Important?\n",
      "\n",
      "AI agents are a foundational concept in AI because they provide a framework for building systems that:\n",
      "\n",
      "*   **Automate complex tasks:** From manufacturing to customer service.\n",
      "*   **Operate in dynamic environments:** Adapting to real-time changes.\n",
      "*   **Process vast amounts of data:** Making informed decisions.\n",
      "*   **Solve problems autonomously:** Without constant human oversight.\n",
      "*   **Enhance human capabilities:** By acting as intelligent assistants or tools.\n",
      "\n",
      "### Modern Examples:\n",
      "\n",
      "*   **Large Language Models (LLMs) used in an \"agentic\" fashion:** When an LLM is given a goal, provided with tools (e.g., web search, code interpreter, API access), and instructed to \"think step-by-step\" to achieve that goal. It perceives a prompt, reasons about tools and steps, and acts by using those tools.\n",
      "*   **Chatbots and Virtual Assistants:** Perceive user input (text/voice), reason about intent, and act by generating responses or performing tasks (e.g., setting a reminder).\n",
      "*   **Robots (e.g., manufacturing robots, autonomous vacuum cleaners):** Perceive their physical environment (sensors), reason about their task/path, and act through motors/effectors.\n",
      "*   **Autonomous Vehicles:** Perceive surroundings (cameras, lidar, radar), process information to build a world model, predict future states, and act by controlling steering, acceleration, and braking.\n",
      "*   **Recommendation Systems:** Perceive user preferences and past behavior, reason about similar items, and act by suggesting content.\n",
      "*   **Game AIs:** Perceive the game state, reason about strategies, and act to control game characters.\n",
      "\n",
      "In summary, AI agents represent the core idea of an intelligent entity that senses, processes information, and acts in its environment to achieve its objectives, forming the basis for much of modern artificial intelligence.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "\n",
    "from google.adk.agents import LlmAgent\n",
    "from google.adk.agents.readonly_context import ReadonlyContext\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.genai import types\n",
    "\n",
    "from config import config\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Configuration\n",
    "# -------------------------------------------------\n",
    "os.environ[\"GOOGLE_API_KEY\"] = config.GOOGLE_API_KEY\n",
    "\n",
    "APP_NAME = \"readonly_context_app\"\n",
    "USER_ID = \"user_1\"\n",
    "SESSION_ID = \"session_1\"\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Dynamic Instruction Provider\n",
    "# -------------------------------------------------\n",
    "def my_instruction_provider(context: ReadonlyContext) -> str:\n",
    "    # Read session state safely\n",
    "    user_tier = context.state.get(\"user_tier\", \"standard\")\n",
    "\n",
    "    return f\"\"\"\n",
    "You are a helpful AI assistant.\n",
    "Process the request for a {user_tier} user.\n",
    "If the user is premium, provide detailed and priority responses.\n",
    "If standard, provide concise helpful answers.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Main Execution\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Create session service\n",
    "session_service = InMemorySessionService()\n",
    "\n",
    "# Create session with initial state\n",
    "session = await session_service.create_session(\n",
    "    app_name=APP_NAME,\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_ID,\n",
    "    state={\"user_tier\": \"premium\"},   # Change to \"standard\" to test\n",
    ")\n",
    "\n",
    "# Create LLM Agent with dynamic instruction provider\n",
    "agent = LlmAgent(\n",
    "    name=\"TierAwareAgent\",\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    instruction=my_instruction_provider,  # Using ReadonlyContext here\n",
    ")\n",
    "\n",
    "# Create runner\n",
    "runner = Runner(\n",
    "    agent=agent,\n",
    "    app_name=APP_NAME,\n",
    "    session_service=session_service,\n",
    ")\n",
    "\n",
    "print(\"\\nSending message...\\n\")\n",
    "\n",
    "# Run agent\n",
    "async for event in runner.run_async(\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_ID,\n",
    "    new_message=types.Content(\n",
    "        role=\"user\",\n",
    "        parts=[types.Part(text=\"Explain what AI agents are.\")],\n",
    "    ),\n",
    "):\n",
    "    if event.content and event.content.parts:\n",
    "        print(\"Agent Response:\\n\")\n",
    "        print(event.content.parts[0].text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9ee518-efd6-46c3-9d9e-f5c7350919bb",
   "metadata": {},
   "source": [
    "### **<font color=\"blue\">CallbackContext</font>**\n",
    "- **Where Used:** Passed as `callback_context` to agent lifecycle callbacks (`before_agent_callback`, `after_agent_callback`) and model interaction callbacks (`before_model_callback`, `after_model_callback`).\n",
    "- **Purpose:** Facilitates inspecting and modifying state, interacting with artifacts, and accessing invocation details specifically within callbacks.\n",
    "- **Key Capabilities (Adds to `ReadonlyContext`):**\n",
    "  - **Mutable `state` Property:** Allows reading and writing to session state. Changes made here (`callback_context.state['key'] = value`) are tracked and associated with the event generated by the framework after the callback.\n",
    "  - **Artifact Methods:** `load_artifact(filename)` and `save_artifact(filename, part)` methods for interacting with the configured `artifact_service`.\n",
    "- Direct `user_content` access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53dc921e-95d7-45ab-bdf1-3139f9a72cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode: Callback receiving CallbackContext\n",
    "from google.adk.agents.callback_context import CallbackContext\n",
    "from google.adk.models import LlmRequest\n",
    "from google.genai import types\n",
    "from typing import Optional\n",
    "\n",
    "def my_before_model_cb(callback_context: CallbackContext, request: LlmRequest) -> Optional[types.Content]:\n",
    "    # Read/Write state example\n",
    "    call_count = callback_context.state.get(\"model_calls\", 0)\n",
    "    callback_context.state[\"model_calls\"] = call_count + 1 # Modify state\n",
    "\n",
    "    # Optionally load an artifact\n",
    "    # config_part = callback_context.load_artifact(\"model_config.json\")\n",
    "    print(f\"Preparing model call #{call_count + 1} for invocation {callback_context.invocation_id}\")\n",
    "    return None # Allow model call to proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "905e53e1-ffcd-4c43-846c-4ba0402fe12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sending message...\n",
      "\n",
      "============================================================\n",
      "Model Call #: 1\n",
      "Invocation ID: e-28580363-b6f6-4b56-bafe-9b4c6498c175\n",
      "Agent Name   : CallbackAgent\n",
      "============================================================\n",
      "User said: What is an AI agent?\n",
      "\n",
      "Agent Response:\n",
      "\n",
      "An **AI agent** is a fundamental concept in artificial intelligence, representing an entity that perceives its environment through sensors and acts upon that environment through actuators.\n",
      "\n",
      "Essentially, an AI agent is designed to achieve specific goals or objectives within a given environment.\n",
      "\n",
      "Here's a breakdown of its key characteristics:\n",
      "\n",
      "1.  **Perception:** Agents gather information from their environment using \"sensors.\" This could be anything from a camera (for a robot), a microphone (for a voice assistant), a text input (for a chatbot), or data feeds (for a trading bot).\n",
      "2.  **Action:** Agents perform actions that affect their environment using \"actuators.\" Examples include moving a robotic arm, speaking a reply, sending a message, or making a trade.\n",
      "3.  **Environment:** This is the context in which the agent operates. It can be physical (e.g., a factory floor, a road for a self-driving car) or digital (e.g., a website, a database, a game world).\n",
      "4.  **Autonomy:** Agents operate to some degree independently, making decisions based on their perceptions and internal state without constant human oversight.\n",
      "5.  **Goals/Objectives:** Agents are built with a purpose. They are designed to achieve specific tasks, optimize outcomes, or make predictions.\n",
      "6.  **Rationality:** A rational agent aims to achieve the best possible outcome given its perceptions, knowledge, and available actions. It tries to maximize its performance measure.\n",
      "7.  **Internal State (Optional but common):** More sophisticated agents maintain an internal representation of their environment, goals, and even their own capabilities. This \"memory\" allows them to plan, learn, and make more informed decisions rather than just reacting to immediate stimuli.\n",
      "\n",
      "**Types of AI Agents (by complexity):**\n",
      "\n",
      "*   **Simple Reflex Agents:** React directly to current perceptions, without memory of past states (e.g., a thermostat turning on/off based on temperature).\n",
      "*   **Model-based Reflex Agents:** Maintain an internal model of the world to help understand how their actions affect the environment and to deal with partially observable environments.\n",
      "*   **Goal-based Agents:** Use their internal model to plan actions that will lead them to a specific goal state.\n",
      "*   **Utility-based Agents:** Go beyond simply achieving goals; they try to achieve the *best* possible outcome, considering the desirability of different states (e.g., balancing risk and reward).\n",
      "*   **Learning Agents:** Are capable of improving their performance over time by learning from their experiences.\n",
      "\n",
      "**Examples:**\n",
      "\n",
      "*   **Robots:** Perceive the physical world with sensors (cameras, lidar) and act with motors and grippers.\n",
      "*   **Chatbots/Virtual Assistants:** Perceive text or speech, process language, and generate text or speech as output.\n",
      "*   **Self-driving Cars:** Perceive roads, traffic, and obstacles, and act by controlling steering, acceleration, and braking.\n",
      "*   **Game AI:** Perceive the game state and act by moving characters, making strategic decisions, or attacking opponents.\n",
      "*   **Recommendation Systems:** Perceive user preferences and past behavior, and act by suggesting products, movies, or articles.\n",
      "\n",
      "In essence, an AI agent is a high-level abstraction for any intelligent system that can sense, think (to some degree), and act in its environment to achieve its objectives. My internal name, \"CallbackAgent,\" is a specific identifier within the system I operate as, indicating I am an AI agent designed to respond to your queries.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from typing import Optional\n",
    "\n",
    "from google.adk.agents import LlmAgent\n",
    "from google.adk.agents.callback_context import CallbackContext\n",
    "from google.adk.models import LlmRequest\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.genai import types\n",
    "\n",
    "from config import config\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Configuration\n",
    "# -------------------------------------------------\n",
    "os.environ[\"GOOGLE_API_KEY\"] = config.GOOGLE_API_KEY\n",
    "\n",
    "APP_NAME = \"callback_context_app\"\n",
    "USER_ID = \"user_1\"\n",
    "SESSION_ID = \"session_1\"\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# BEFORE MODEL CALLBACK (Correct Signature)\n",
    "# -------------------------------------------------\n",
    "def my_before_model_cb(\n",
    "    callback_context: CallbackContext,\n",
    "    llm_request: LlmRequest,\n",
    ") -> Optional[types.Content]:\n",
    "\n",
    "    # ✅ Read state\n",
    "    call_count = callback_context.state.get(\"model_calls\", 0)\n",
    "\n",
    "    # ✅ Write state\n",
    "    callback_context.state[\"model_calls\"] = call_count + 1\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Model Call #: {call_count + 1}\")\n",
    "    print(f\"Invocation ID: {callback_context.invocation_id}\")\n",
    "    print(f\"Agent Name   : {callback_context.agent_name}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # ✅ Direct user content access\n",
    "    if callback_context.user_content:\n",
    "        print(\"User said:\", callback_context.user_content.parts[0].text)\n",
    "\n",
    "    # Returning None allows model call to proceed\n",
    "    return None\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Main Execution\n",
    "# -------------------------------------------------\n",
    "session_service = InMemorySessionService()\n",
    "\n",
    "await session_service.create_session(\n",
    "    app_name=APP_NAME,\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_ID,\n",
    "    state={\"model_calls\": 0},\n",
    ")\n",
    "\n",
    "agent = LlmAgent(\n",
    "    name=\"CallbackAgent\",\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    instruction=\"You are a helpful assistant.\",\n",
    "    before_model_callback=my_before_model_cb,\n",
    ")\n",
    "\n",
    "runner = Runner(\n",
    "    agent=agent,\n",
    "    app_name=APP_NAME,\n",
    "    session_service=session_service,\n",
    ")\n",
    "\n",
    "print(\"\\nSending message...\\n\")\n",
    "\n",
    "async for event in runner.run_async(\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_ID,\n",
    "    new_message=types.Content(\n",
    "        role=\"user\",\n",
    "        parts=[types.Part(text=\"What is an AI agent?\")],\n",
    "    ),\n",
    "):\n",
    "    if event.content and event.content.parts:\n",
    "        print(\"\\nAgent Response:\\n\")\n",
    "        print(event.content.parts[0].text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f9761b-132d-4ec1-b34d-289a1f849629",
   "metadata": {},
   "source": [
    "### **<font color=\"blue\">ToolContext</font>**\n",
    "- **Where Used:** Passed as `tool_context` to the functions backing `FunctionTools` and to tool execution callbacks (`before_tool_callback`, `after_tool_callback`).\n",
    "- **Purpose:** Provides everything `CallbackContext` does, plus specialized methods essential for tool execution, like handling authentication, searching memory, and listing artifacts.\n",
    "- **Key Capabilities (Adds to `CallbackContext`):**\n",
    "  - **Authentication Methods:** `request_credential(auth_config)` to trigger an auth flow, and `get_auth_response(auth_config)` to retrieve credentials provided by the user/system.\n",
    "  - **Artifact Listing:** `list_artifacts()` to discover available artifacts in the session.\n",
    "  - **Memory Search:** `search_memory(query)` to query the configured `memory_service`.\n",
    "  - **`function_call_id` Property:** Identifies the specific function call from the LLM that triggered this tool execution, crucial for linking authentication requests or responses back correctly.\n",
    "  - **`actions` Property:** Direct access to the `EventActions` object for this step, allowing the tool to signal state changes, auth requests, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff4dc1d2-076b-46d0-8587-0b78002df890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode: Tool function receiving ToolContext\n",
    "from google.adk.tools import ToolContext\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Assume this function is wrapped by a FunctionTool\n",
    "def search_external_api(query: str, tool_context: ToolContext) -> Dict[str, Any]:\n",
    "    api_key = tool_context.state.get(\"api_key\")\n",
    "    if not api_key:\n",
    "        # Define required auth config\n",
    "        # auth_config = AuthConfig(...)\n",
    "        # tool_context.request_credential(auth_config) # Request credentials\n",
    "        # Use the 'actions' property to signal the auth request has been made\n",
    "        # tool_context.actions.requested_auth_configs[tool_context.function_call_id] = auth_config\n",
    "        return {\"status\": \"Auth Required\"}\n",
    "\n",
    "    # Use the API key...\n",
    "    print(f\"Tool executing for query '{query}' using API key. Invocation: {tool_context.invocation_id}\")\n",
    "\n",
    "    # Optionally search memory or list artifacts\n",
    "    # relevant_docs = tool_context.search_memory(f\"info related to {query}\")\n",
    "    # available_files = tool_context.list_artifacts()\n",
    "\n",
    "    return {\"result\": f\"Data for {query} fetched.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48a8a22e-b476-45d0-8ac3-50802be97e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent Response:\n",
      "\n",
      "None\n",
      "\n",
      "Agent Response:\n",
      "\n",
      "None\n",
      "\n",
      "Agent Response:\n",
      "\n",
      "None\n",
      "\n",
      "Agent Response:\n",
      "\n",
      "I'm sorry, I cannot fulfill this request. The financial market data is secure and requires an API key to access. It seems that the API key is missing or invalid.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from typing import Dict, Any\n",
    "\n",
    "from google.adk.agents import LlmAgent\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.tools import FunctionTool, ToolContext\n",
    "from google.adk.auth import AuthConfig\n",
    "from google.genai import types\n",
    "\n",
    "from config import config\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = config.GOOGLE_API_KEY\n",
    "\n",
    "APP_NAME = \"enterprise_auth_app\"\n",
    "USER_ID = \"user_1\"\n",
    "SESSION_ID = \"session_1\"\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Tool Function\n",
    "# -------------------------------------------------\n",
    "def search_external_api(\n",
    "    query: str,\n",
    "    tool_context: ToolContext\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Search secure external API data using a query string.\n",
    "    Requires an API key.\n",
    "    \"\"\"\n",
    "\n",
    "    api_key = tool_context.state.get(\"external_api_key\")\n",
    "\n",
    "    # If no API key → trigger ADK Credential Request\n",
    "    if not api_key:\n",
    "        auth_config = AuthConfig(\n",
    "            authScheme={\n",
    "                \"type\": \"apiKey\",\n",
    "                \"in\": \"header\",\n",
    "                \"name\": \"X-API-Key\"\n",
    "            },\n",
    "            description=\"Please provide your External API Key.\"\n",
    "        )\n",
    "\n",
    "        tool_context.request_credential(auth_config)\n",
    "\n",
    "        return {\"status\": \"Authentication Required\"}\n",
    "\n",
    "    # If API key exists\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Using API Key: {api_key}\")\n",
    "    print(f\"Invocation ID: {tool_context.invocation_id}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    return {\n",
    "        \"result\": f\"Secure data fetched for query '{query}'.\"\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Main Async Runner\n",
    "# -------------------------------------------------\n",
    "session_service = InMemorySessionService()\n",
    "\n",
    "await session_service.create_session(\n",
    "    app_name=APP_NAME,\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_ID,\n",
    ")\n",
    "\n",
    "search_tool = FunctionTool(search_external_api)\n",
    "\n",
    "agent = LlmAgent(\n",
    "    name=\"EnterpriseAgent\",\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    instruction=\"\"\"\n",
    "If user asks to search secure data,\n",
    "call the search_external_api tool.\n",
    "\"\"\",\n",
    "    tools=[search_tool],\n",
    ")\n",
    "\n",
    "runner = Runner(\n",
    "    agent=agent,\n",
    "    app_name=APP_NAME,\n",
    "    session_service=session_service,\n",
    ")\n",
    "\n",
    "async for event in runner.run_async(\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_ID,\n",
    "    new_message=types.Content(\n",
    "        role=\"user\",\n",
    "        parts=[types.Part(text=\"Search financial market data\")],\n",
    "    ),\n",
    "):\n",
    "    if event.content and event.content.parts:\n",
    "        print(\"\\nAgent Response:\\n\")\n",
    "        print(event.content.parts[0].text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d51e5d2-2d94-490f-9787-c1ad348f659f",
   "metadata": {},
   "source": [
    "## **<font color=\"red\">Configure context caching</font>**\n",
    "- You configure the context caching feature at the ADK `App` object level, which wraps your agent. Use the `ContextCacheConfig` class to configure these settings, as shown in the following code sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f597c3f4-5e60-4031-ab6b-721b1be4f3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sending message...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_14056\\3186723127.py:42: UserWarning: [EXPERIMENTAL] ContextCacheConfig: This feature is experimental and may change or be removed in future versions without notice. It may introduce breaking changes at any time.\n",
      "  context_cache_config=ContextCacheConfig(\n",
      "D:\\Agent-Development-Kit\\venv\\Lib\\site-packages\\google\\adk\\models\\google_llm.py:172: UserWarning: [EXPERIMENTAL] GeminiContextCacheManager: This feature is experimental and may change or be removed in future versions without notice. It may introduce breaking changes at any time.\n",
      "  cache_manager = GeminiContextCacheManager(self.api_client)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent Response:\n",
      "\n",
      "The Transformer architecture, introduced in the 2017 paper \"Attention Is All You Need\" by Vaswani et al., revolutionized the field of natural language processing (NLP) and sequence modeling. Before Transformers, recurrent neural networks (RNNs) and their variants (LSTMs, GRUs) were dominant for sequence tasks. Transformers achieved superior performance by replacing recurrence with an **attention mechanism**, allowing for highly parallel computation and better handling of long-range dependencies.\n",
      "\n",
      "Let's break down how Transformers work in detail.\n",
      "\n",
      "---\n",
      "\n",
      "### I. The Core Idea: Attention\n",
      "\n",
      "The fundamental innovation of the Transformer is the **attention mechanism**. Instead of processing a sequence word by word (like RNNs), attention allows the model to weigh the importance of different words in the input sequence when processing each word.\n",
      "\n",
      "Imagine you're translating the sentence \"The animal didn't cross the street because it was too tired.\" When deciding what \"it\" refers to, you'd naturally look back at \"animal\" and \"street.\" An attention mechanism does something similar: for each word, it looks at all other words in the input and assigns them a \"score\" of relevance.\n",
      "\n",
      "### II. Overall Architecture: Encoder-Decoder\n",
      "\n",
      "A standard Transformer model has an **Encoder-Decoder** structure, similar to many sequence-to-sequence models.\n",
      "\n",
      "1.  **Encoder:** Processes the input sequence (e.g., a sentence in English). It maps an input sequence of symbol representations $(x_1, ..., x_n)$ to a sequence of continuous representations $(z_1, ..., z_n)$.\n",
      "2.  **Decoder:** Takes the encoder's output and generates an output sequence (e.g., the translated sentence in French). Given $(z_1, ..., z_n)$, the decoder generates an output sequence $(y_1, ..., y_m)$ one element at a time.\n",
      "\n",
      "Both the Encoder and Decoder are composed of a stack of identical layers.\n",
      "\n",
      "---\n",
      "\n",
      "### III. The Encoder\n",
      "\n",
      "The Encoder is responsible for understanding the input sequence. It consists of `N` identical layers (e.g., N=6 in the original paper). Each layer has two main sub-layers:\n",
      "\n",
      "1.  **Multi-Head Self-Attention Mechanism:** Allows the encoder to weigh the importance of different words in the *input sequence itself* when processing a specific word.\n",
      "2.  **Position-wise Feed-Forward Network:** A simple fully connected feed-forward network applied independently to each position.\n",
      "\n",
      "Crucially, each sub-layer is followed by a **Residual Connection** and **Layer Normalization**.\n",
      "\n",
      "#### A. Input Embedding\n",
      "\n",
      "First, input tokens (words) are converted into numerical representations called **embeddings**. These embeddings are dense vectors that capture semantic meaning.\n",
      "\n",
      "#### B. Positional Encoding\n",
      "\n",
      "Unlike RNNs, Transformers process all input tokens in parallel. This means they have no inherent understanding of the *order* of words in a sequence. To address this, **Positional Encodings** are added to the input embeddings.\n",
      "\n",
      "*   These are vectors that carry information about the position of each word.\n",
      "*   They have the same dimension as the word embeddings, so they can be simply summed.\n",
      "*   The original paper uses sine and cosine functions of different frequencies to generate these encodings, allowing the model to learn to attend to relative positions.\n",
      "\n",
      "#### C. Multi-Head Self-Attention (in Encoder)\n",
      "\n",
      "This is the heart of the Transformer. For each word in the input sequence, self-attention calculates a weighted sum of all words in the sequence.\n",
      "\n",
      "Let's break down **Self-Attention** first, then **Multi-Head**:\n",
      "\n",
      "**1. Scaled Dot-Product Attention:**\n",
      "The core operation is defined by three learned matrices derived from the input embeddings:\n",
      "*   **Query (Q):** Represents what we are looking for (the current word's query).\n",
      "*   **Key (K):** Represents what information is available at other positions (other words' keys).\n",
      "*   **Value (V):** Represents the actual content to extract if a key matches a query (other words' values).\n",
      "\n",
      "For each word embedding $x_i$:\n",
      "*   $Q_i = x_i W_Q$\n",
      "*   $K_i = x_i W_K$\n",
      "*   $V_i = x_i W_V$\n",
      "Where $W_Q, W_K, W_V$ are learned weight matrices.\n",
      "\n",
      "The attention score for a given Query $Q$ (from a specific word) against all Keys $K$ (from all words) is calculated as:\n",
      "$Attention(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$\n",
      "\n",
      "*   $QK^T$: Dot product between the query and all keys. This measures how well each key matches the query.\n",
      "*   $\\sqrt{d_k}$: The scaling factor (where $d_k$ is the dimension of the keys). This prevents dot products from becoming too large, pushing the softmax into regions with extremely small gradients.\n",
      "*   Softmax: Normalizes the scores into probabilities, indicating how much attention to pay to each word.\n",
      "*   $V$: These probabilities are then multiplied by the Value vectors, effectively taking a weighted sum of the values. Words with higher attention scores contribute more to the output.\n",
      "\n",
      "**2. Multi-Head Attention:**\n",
      "Instead of performing a single attention function, Multi-Head Attention allows the model to jointly attend to information from different \"representation subspaces\" at different positions.\n",
      "\n",
      "*   The Q, K, and V matrices are linearly projected `h` times (e.g., 8 times) with different learned linear projections.\n",
      "*   For each of these `h` \"heads,\" a scaled dot-product attention is computed.\n",
      "*   The output of all `h` attention heads are then concatenated and linearly projected back into the desired output dimension.\n",
      "This allows the model to capture diverse types of relationships (e.g., one head might focus on syntactic relationships, another on semantic relationships).\n",
      "\n",
      "#### D. Position-wise Feed-Forward Network\n",
      "\n",
      "After the Multi-Head Self-Attention sub-layer, the output passes through a simple, fully connected feed-forward network. This network is applied identically and independently to each position (word). It typically consists of two linear transformations with a ReLU activation in between:\n",
      "$FFN(x) = \\text{max}(0, xW_1 + b_1)W_2 + b_2$\n",
      "\n",
      "#### E. Residual Connections & Layer Normalization\n",
      "\n",
      "*   **Residual Connections:** After each sub-layer (self-attention and feed-forward), a residual connection adds the sub-layer's input to its output. This helps with training very deep networks by allowing gradients to flow more easily.\n",
      "    *   `Output = LayerNorm(x + Sublayer(x))`\n",
      "*   **Layer Normalization:** Normalizes the activations across the features for each sample in a batch. This stabilizes training and speeds convergence.\n",
      "\n",
      "---\n",
      "\n",
      "### IV. The Decoder\n",
      "\n",
      "The Decoder is also composed of `N` identical layers. However, each Decoder layer has three main sub-layers:\n",
      "\n",
      "1.  **Masked Multi-Head Self-Attention:** Similar to the encoder's self-attention, but with a crucial modification.\n",
      "2.  **Multi-Head Encoder-Decoder Attention:** Attends to the output of the encoder.\n",
      "3.  **Position-wise Feed-Forward Network:** Same as in the encoder.\n",
      "\n",
      "Again, each sub-layer is followed by a Residual Connection and Layer Normalization.\n",
      "\n",
      "#### A. Output Embeddings & Positional Encoding\n",
      "\n",
      "Similar to the encoder, the decoder receives target sequence embeddings (e.g., partial translation) and positional encodings.\n",
      "\n",
      "#### B. Masked Multi-Head Self-Attention (in Decoder)\n",
      "\n",
      "This sub-layer ensures that the predictions for position $i$ can only depend on the known outputs at positions less than $i$. This is crucial during training, as it prevents the model from \"cheating\" by looking at future tokens in the target sequence.\n",
      "\n",
      "*   It works by setting the attention scores for future positions to negative infinity *before* the softmax step. This effectively zeroes out any attention to future tokens.\n",
      "\n",
      "#### C. Multi-Head Encoder-Decoder Attention\n",
      "\n",
      "This is where the decoder interacts with the encoder's output.\n",
      "*   The **Queries (Q)** come from the *previous decoder layer's output*.\n",
      "*   The **Keys (K)** and **Values (V)** come from the *output of the encoder stack*.\n",
      "This allows the decoder to focus on relevant parts of the input sentence when generating the output at each step.\n",
      "\n",
      "#### D. Position-wise Feed-Forward Network, Residuals, and Layer Normalization\n",
      "\n",
      "These are identical to their counterparts in the encoder.\n",
      "\n",
      "#### E. Output Layer (Linear and Softmax)\n",
      "\n",
      "The final output of the decoder stack is passed through a linear layer, which projects it to a higher-dimensional vector (the size of the vocabulary). Then, a softmax function converts these scores into probabilities for each word in the vocabulary, yielding the next predicted word.\n",
      "\n",
      "---\n",
      "\n",
      "### V. Training the Transformer\n",
      "\n",
      "*   **Objective:** To learn to predict the next word in the target sequence given the previous words and the encoded input sequence.\n",
      "*   **Loss Function:** Typically, **cross-entropy loss** is used, comparing the predicted probability distribution of the next word with the actual next word.\n",
      "*   **Teacher Forcing:** During training, the decoder is given the *actual* previous target tokens as input (not its own predictions). This speeds up training and makes it more stable.\n",
      "*   **Optimization:** Often uses adaptive learning rate optimizers like Adam, with a warm-up phase.\n",
      "\n",
      "---\n",
      "\n",
      "### VI. Key Innovations & Advantages of Transformers\n",
      "\n",
      "1.  **Parallelization:** The attention mechanism processes all tokens simultaneously, unlike RNNs which are sequential. This makes training significantly faster on modern hardware (GPUs, TPUs).\n",
      "2.  **Long-Range Dependencies:** By directly attending to any word in the sequence, Transformers can capture long-range dependencies much more effectively than RNNs, which suffer from vanishing/exploding gradients over long sequences.\n",
      "3.  **Contextual Embeddings:** Each word's representation is dynamically calculated based on its context in the sentence, leading to richer and more nuanced embeddings.\n",
      "4.  **Transfer Learning:** The pre-training of large Transformer models on vast amounts of text data (e.g., BERT, GPT, T5) has enabled powerful transfer learning to downstream tasks with much less task-specific data.\n",
      "\n",
      "### VII. Disadvantages/Limitations\n",
      "\n",
      "1.  **Computational Cost:** The attention mechanism scales quadratically with the sequence length ($O(L^2)$) in terms of memory and computation, which can be a bottleneck for very long sequences.\n",
      "2.  **Lack of Inductive Bias:** Unlike CNNs (locality, translation invariance) or RNNs (sequential processing), Transformers are \"blank slates.\" While powerful, this means they need vast amounts of data to learn basic sequential or hierarchical relationships. Positional encodings help, but don't fully solve this.\n",
      "3.  **Interpretability:** While attention weights can offer some insights into what the model is focusing on, the full complexity of multi-head attention and deep networks still makes them somewhat black boxes.\n",
      "\n",
      "---\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "The Transformer architecture is a highly influential and powerful model due to its reliance on the attention mechanism, which allows for parallel processing, effective capture of long-range dependencies, and rich contextual representations. It has become the backbone for state-of-the-art models in NLP, computer vision (Vision Transformers), and other domains involving sequence data.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "\n",
    "from google.adk import Agent\n",
    "from google.adk.apps.app import App\n",
    "from google.adk.agents.context_cache_config import ContextCacheConfig\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.genai import types\n",
    "\n",
    "from config import config\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Configuration\n",
    "# -------------------------------------------------\n",
    "os.environ[\"GOOGLE_API_KEY\"] = config.GOOGLE_API_KEY\n",
    "\n",
    "APP_NAME = \"my_caching_agent_app\"\n",
    "USER_ID = \"user_1\"\n",
    "SESSION_ID = \"session_1\"\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1️⃣ Create Root Agent\n",
    "# -------------------------------------------------\n",
    "root_agent = Agent(\n",
    "    name=\"CachingAgent\",\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    instruction=\"\"\"\n",
    "You are a helpful AI assistant.\n",
    "Provide detailed explanations when appropriate.\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2️⃣ Create App with Context Caching\n",
    "# -------------------------------------------------\n",
    "app = App(\n",
    "    name=APP_NAME,   # Must be valid identifier\n",
    "    root_agent=root_agent,\n",
    "    context_cache_config=ContextCacheConfig(\n",
    "        min_tokens=2048,\n",
    "        ttl_seconds=600,\n",
    "        cache_intervals=5,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3️⃣ Run the App\n",
    "# -------------------------------------------------\n",
    "session_service = InMemorySessionService()\n",
    "\n",
    "await session_service.create_session(\n",
    "    app_name=APP_NAME,\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_ID,\n",
    ")\n",
    "\n",
    "runner = Runner(\n",
    "    app=app,  # ⚠ Important: pass App\n",
    "    session_service=session_service,\n",
    ")\n",
    "\n",
    "print(\"\\nSending message...\\n\")\n",
    "\n",
    "async for event in runner.run_async(\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_ID,\n",
    "    new_message=types.Content(\n",
    "        role=\"user\",\n",
    "        parts=[types.Part(\n",
    "            text=\"Explain in detail how transformers work.\"\n",
    "        )],\n",
    "    ),\n",
    "):\n",
    "    if event.content and event.content.parts:\n",
    "        print(\"\\nAgent Response:\\n\")\n",
    "        print(event.content.parts[0].text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23349bea-ba0d-42e6-91de-51677eb04628",
   "metadata": {},
   "source": [
    "## **<font color='red'>Compress agent context for performance</font>**\n",
    "- As an ADK agent runs it collects _context_ information, including user instructions, retrieved data, tool responses, and generated content. As the size of this context data grows, agent processing times typically also increase. More and more data is sent to the generative AI model used by the agent, increasing processing time and slowing down responses. The ADK Context Compaction feature is designed to reduce the size of context as an agent is running by summarizing older parts of the agent workflow event history.\n",
    "- The Context Compaction feature uses a sliding window approach for collecting and summarizing agent workflow event data within a __Session__. When you configure this feature in your agent, it summarizes data from older events once it reaches a threshold of a specific number of workflow events, or invocations, with the current Session.\n",
    "- Add context compaction to your agent workflow by adding an Events Compaction Configuration setting to the App object of your workflow. As part of the configuration, you must specify a compaction interval and overlap size, as shown in the following sample code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec362dcf-d4e4-4bac-94f4-9336a4679494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_14056\\3827632384.py:15: UserWarning: [EXPERIMENTAL] EventsCompactionConfig: This feature is experimental and may change or be removed in future versions without notice. It may introduce breaking changes at any time.\n",
      "  events_compaction_config=EventsCompactionConfig(\n"
     ]
    }
   ],
   "source": [
    "from google.adk.apps.app import App, EventsCompactionConfig\n",
    "from google.adk.apps.llm_event_summarizer import LlmEventSummarizer\n",
    "from google.adk.models import Gemini\n",
    "\n",
    "# Define the AI model to be used for summarization:\n",
    "summarization_llm = Gemini(model=\"gemini-2.5-flash\")\n",
    "\n",
    "# Create the summarizer with the custom model:\n",
    "my_summarizer = LlmEventSummarizer(llm=summarization_llm)\n",
    "\n",
    "# Configure the App with the custom summarizer and compaction settings:\n",
    "app = App(\n",
    "    name=APP_NAME,\n",
    "    root_agent=root_agent,\n",
    "    events_compaction_config=EventsCompactionConfig(\n",
    "        compaction_interval=3,\n",
    "        overlap_size=1,\n",
    "        summarizer=my_summarizer,\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0319b2dc-e222-44bd-9570-384e46acf2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Conversation...\n",
      "\n",
      "====================================================================================================\n",
      "User: Explain what AI agents are.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_14056\\193324168.py:55: UserWarning: [EXPERIMENTAL] EventsCompactionConfig: This feature is experimental and may change or be removed in future versions without notice. It may introduce breaking changes at any time.\n",
      "  events_compaction_config=EventsCompactionConfig(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent:\n",
      "Okay, let's break down AI Agents.\n",
      "\n",
      "**What is an AI Agent?**\n",
      "\n",
      "At its core, an AI agent is an autonomous entity that perceives its environment through sensors and acts upon that environment through actuators to achieve a specific goal. Think of it as a computer program that can make decisions and take actions independently.\n",
      "\n",
      "Here's a more detailed breakdown of the key aspects:\n",
      "\n",
      "*   **Autonomy:** AI agents operate without direct, continuous human intervention. They can make decisions and take actions on their own, based on their programming and the data they receive.\n",
      "*   **Perception:** They perceive their environment using sensors. These sensors can be anything from cameras and microphones to data feeds and APIs. The agent uses this sensory input to understand the current state of its environment.\n",
      "*   **Action:** AI agents act upon their environment through actuators. Actuators are the mechanisms that allow the agent to perform actions, such as moving a robot arm, sending an email, or adjusting a thermostat.\n",
      "*   **Goal-Oriented:** AI agents are designed to achieve specific goals. These goals can be simple or complex, and the agent's actions are guided by the desire to reach those goals.\n",
      "*   **Intelligence:** Agents use AI techniques (like machine learning, rule-based systems, or planning algorithms) to process information, learn from experience, and make intelligent decisions.\n",
      "*    **Reasoning:** Agents can analyze situations and find the best possible solution using logical reasoning.\n",
      "\n",
      "**Key Components of an AI Agent:**\n",
      "\n",
      "1.  **Environment:** The world in which the agent operates. The environment provides the agent with sensory input and is affected by the agent's actions.\n",
      "2.  **Sensors:** Devices or mechanisms that allow the agent to perceive its environment.\n",
      "3.  **Actuators:** Devices or mechanisms that allow the agent to act upon its environment.\n",
      "4.  **Agent Function:** The core logic that maps the agent's perceptions to its actions. This function determines how the agent will behave in any given situation.\n",
      "\n",
      "**Types of AI Agents:**\n",
      "\n",
      "AI agents can be categorized in various ways, based on their complexity, capabilities, and how they make decisions. Here are a few common categories:\n",
      "\n",
      "*   **Simple Reflex Agents:** These are the simplest type of agent. They react directly to their perceptions, based on a set of pre-defined rules. They have no memory of past states.  *Example: A thermostat that turns on the heat when the temperature drops below a certain point.*\n",
      "\n",
      "*   **Model-Based Reflex Agents:** These agents maintain an internal model of the environment, which allows them to reason about the consequences of their actions.  *Example: An automated vacuum cleaner that uses sensors to map the room and avoid obstacles.*\n",
      "\n",
      "*   **Goal-Based Agents:** These agents have a specific goal in mind and try to take actions that will lead them to that goal. They use search and planning algorithms to find the best sequence of actions.  *Example: A navigation app that finds the shortest route to a destination.*\n",
      "\n",
      "*   **Utility-Based Agents:** These agents go beyond simply achieving a goal. They also try to maximize their \"utility,\" which is a measure of how desirable a particular state of the environment is. They make decisions based on what will make them \"happiest.\" *Example: A trading bot that tries to maximize profit while minimizing risk.*\n",
      "\n",
      "*   **Learning Agents:** These agents can learn from their experiences and improve their performance over time. They use machine learning algorithms to adapt to new situations and refine their decision-making process. *Example: A spam filter that learns to identify new types of spam based on user feedback.*\n",
      "\n",
      "**Examples of AI Agents in Action:**\n",
      "\n",
      "*   **Self-driving cars:** Perceive the road and surrounding vehicles, and take actions to steer, accelerate, and brake.\n",
      "*   **Chatbots:** Perceive user input and respond with relevant information or actions.\n",
      "*   **Recommendation systems:** Perceive user preferences and recommend products or services.\n",
      "*   **Robotics:** Perform tasks in manufacturing, healthcare, or other industries.\n",
      "*   **Smart home devices:** Control lighting, temperature, and security systems based on user preferences and environmental conditions.\n",
      "*   **Game playing AI:**  Like AlphaGo, perceive the board state and make moves to win the game.\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "AI agents are intelligent, autonomous entities that can perceive their environment, make decisions, and take actions to achieve specific goals. They are a powerful tool for automating tasks, solving complex problems, and creating intelligent systems. Their ability to operate independently and adapt to changing conditions makes them valuable in a wide range of applications.\n",
      "\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "User: How do they use memory?\n",
      "\n",
      "Agent:\n",
      "Great question! How AI agents use memory is crucial to their intelligence and ability to operate effectively in complex environments. Here's a breakdown of how different types of AI agents leverage memory:\n",
      "\n",
      "**1. Types of Memory in AI Agents:**\n",
      "\n",
      "*   **Short-Term Memory (STM) / Working Memory:**\n",
      "    *   This is temporary storage for information that the agent is currently processing.\n",
      "    *   It holds sensory input, intermediate calculations, and the results of recent actions.\n",
      "    *   Limited capacity and fast access.  Think of it like RAM in a computer.\n",
      "    *   Used for immediate decision-making and action execution.\n",
      "    *   *Example:* In a self-driving car, STM might hold the current speed, the distance to the car in front, and the status of the traffic light.\n",
      "\n",
      "*   **Long-Term Memory (LTM):**\n",
      "    *   This is persistent storage for knowledge, beliefs, goals, and learned experiences.\n",
      "    *   Larger capacity than STM, but slower access. Think of it like the hard drive in a computer.\n",
      "    *   Used for reasoning, planning, and learning.\n",
      "    *   *Example:* In a chatbot, LTM might store information about user preferences, common questions and answers, and the rules of the conversation.\n",
      "\n",
      "*   **Episodic Memory:**\n",
      "    *   A type of LTM that stores specific events or episodes that the agent has experienced, including the context (time, place, emotions).\n",
      "    *   Helps the agent learn from past mistakes and successes, and to generalize to new situations.\n",
      "    *   *Example:* A robot might remember a specific instance where it tried to grasp an object and failed, and use that memory to improve its grasping technique.\n",
      "\n",
      "*   **Semantic Memory:**\n",
      "    *   A type of LTM that stores general knowledge, facts, and concepts about the world.\n",
      "    *   Independent of specific experiences.\n",
      "    *   *Example:* A chatbot might know that \"Paris\" is the capital of France, even if it has never been there.\n",
      "\n",
      "**2. How Different Agent Architectures Use Memory:**\n",
      "\n",
      "*   **Simple Reflex Agents:**\n",
      "    *   **Minimal memory:** These agents primarily rely on immediate perceptions and a set of pre-defined rules. They have little to no memory of past states or actions.\n",
      "    *   **Limited effectiveness:** Their lack of memory makes them unsuitable for complex environments where past actions influence future outcomes.\n",
      "\n",
      "*   **Model-Based Reflex Agents:**\n",
      "    *   **Model of the world:** They maintain an internal model of the environment, which represents the current state of the world and how it changes over time.  This model acts as a form of memory.\n",
      "    *   **State representation:** The model stores information about the relevant aspects of the environment, allowing the agent to reason about the consequences of its actions.\n",
      "\n",
      "*   **Goal-Based Agents:**\n",
      "    *   **Goal representation:** They store their desired goals and use search and planning algorithms to find sequences of actions that will achieve those goals.\n",
      "    *   **Search space:** They may also store information about the search space, such as the cost of different actions or the estimated distance to the goal.\n",
      "\n",
      "*   **Utility-Based Agents:**\n",
      "    *   **Utility function:** They store a utility function that represents their preferences and values. The utility function is used to evaluate the desirability of different states of the environment.\n",
      "    *   **Preference learning:** They may also learn about user preferences over time and update their utility function accordingly.\n",
      "\n",
      "*   **Learning Agents:**\n",
      "    *   **Experience replay:** Learning agents use memory extensively to store past experiences (observations, actions, rewards).\n",
      "    *   **Model training:** These experiences are then used to train machine learning models, which allow the agent to improve its performance over time.\n",
      "    *   **Examples:**\n",
      "        *   *Reinforcement Learning (RL):*  Stores past state-action-reward sequences to learn an optimal policy. The \"replay buffer\" is a key memory component.\n",
      "        *   *Deep Learning Models (e.g., RNNs, LSTMs, Transformers):*  These architectures are designed to process sequential data and maintain hidden states that capture information about the past.  They are, in essence, powerful memory mechanisms.\n",
      "\n",
      "**3. Techniques for Implementing Memory in AI Agents:**\n",
      "\n",
      "*   **Data Structures:** Arrays, lists, dictionaries, trees, and graphs are used to store information about the environment, goals, and past experiences.\n",
      "\n",
      "*   **Databases:** More complex agents may use databases to store large amounts of data about the environment or user preferences.\n",
      "\n",
      "*   **Knowledge Representation:** Techniques like semantic networks, ontologies, and rule-based systems are used to represent knowledge in a structured and reusable way.\n",
      "\n",
      "*   **Recurrent Neural Networks (RNNs) and LSTMs:** These neural network architectures are specifically designed to process sequential data and maintain internal states that capture information about the past. They are commonly used in natural language processing and time series analysis.\n",
      "\n",
      "*   **Transformers:** Attention-based models that can effectively capture long-range dependencies in sequential data. They have become the dominant architecture in many NLP tasks.\n",
      "\n",
      "**In summary:**  AI agents use various forms of memory to store and retrieve information about their environment, goals, past experiences, and learned knowledge. The type of memory used depends on the complexity of the agent and the task it is designed to perform. Learning agents are particularly reliant on memory, as they use it to learn from past experiences and improve their performance over time. The choice of memory architecture is a critical factor in the design of an effective AI agent.\n",
      "\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "User: What is event compaction?\n",
      "\n",
      "Agent:\n",
      "Okay, let's explore the concept of event compaction, especially in the context of AI agents and their memory systems.\n",
      "\n",
      "**What is Event Compaction?**\n",
      "\n",
      "Event compaction is a technique used to reduce the amount of memory required to store historical event data, while still preserving essential information for analysis, learning, or decision-making. It involves summarizing, filtering, or aggregating events to create a more compact representation of past activity.  The goal is to balance memory usage with the need to retain information relevant to the agent's tasks.\n",
      "\n",
      "**Why is Event Compaction Important for AI Agents?**\n",
      "\n",
      "*   **Memory Limitations:** AI agents, especially those that operate in complex and dynamic environments, can generate vast amounts of event data. Storing all of this data can quickly exhaust available memory resources.\n",
      "\n",
      "*   **Efficiency:** Processing large volumes of event data can be computationally expensive, slowing down the agent's response time and hindering its ability to learn and adapt.\n",
      "\n",
      "*   **Long-Term Learning:** For agents that need to learn from past experiences over extended periods, storing detailed event histories may be impractical. Event compaction allows the agent to retain a compressed version of its history for long-term learning and adaptation.\n",
      "\n",
      "*   **Focus on Relevant Information:** Not all events are equally important. Event compaction allows the agent to focus on the most relevant events, filtering out noise or irrelevant details.\n",
      "\n",
      "**Techniques for Event Compaction:**\n",
      "\n",
      "Here are some common methods for event compaction:\n",
      "\n",
      "1.  **Aggregation/Summarization:**\n",
      "    *   **Principle:** Combine multiple similar events into a single, representative event.\n",
      "    *   **Example:** Instead of storing every individual temperature reading from a sensor, store the average temperature over a specific time interval (e.g., hourly average).\n",
      "    *   **Use Case:** Time-series data, sensor data analysis, monitoring systems.\n",
      "    *   **Trade-off:** Loss of fine-grained detail.\n",
      "\n",
      "2.  **Filtering/Selection:**\n",
      "    *   **Principle:** Discard events that are deemed irrelevant or redundant based on predefined criteria.\n",
      "    *   **Example:**  In a manufacturing process, only store events related to equipment failures or quality control issues, discarding routine operational data.\n",
      "    *   **Use Case:** Anomaly detection, fault diagnosis, security monitoring.\n",
      "    *   **Trade-off:** Risk of discarding information that may be useful in the future.\n",
      "\n",
      "3.  **Change-Based Recording:**\n",
      "    *   **Principle:** Only store events when a significant change occurs in the environment or the agent's state.\n",
      "    *   **Example:** A smart thermostat might only record events when the temperature changes by more than a certain threshold, or when the user adjusts the settings.\n",
      "    *   **Use Case:** Control systems, robotics, smart home applications.\n",
      "    *   **Trade-off:** May miss subtle but important trends.\n",
      "\n",
      "4.  **Hierarchical Event Representation:**\n",
      "    *   **Principle:** Organize events into a hierarchy, with more detailed information at lower levels and more abstract summaries at higher levels.\n",
      "    *   **Example:** In a customer service chatbot, individual messages might be grouped into conversations, which are then grouped into interaction types (e.g., complaint, inquiry, feedback).\n",
      "    *   **Use Case:** Natural language processing, dialogue systems, customer relationship management.\n",
      "    *   **Trade-off:** Increased complexity in event processing.\n",
      "\n",
      "5.  **Temporal Abstraction:**\n",
      "    *   **Principle:** Replace sequences of events with higher-level concepts that represent the overall pattern of activity.\n",
      "    *   **Example:** A sequence of \"walk forward,\" \"turn right,\" \"walk forward\" events might be abstracted to \"move to location X.\"\n",
      "    *   **Use Case:** Activity recognition, plan recognition, robotics.\n",
      "    *   **Trade-off:** Loss of temporal precision.\n",
      "\n",
      "6.  **Sketching and Summarization Data Structures:**\n",
      "    *   **Principle:** Use specialized data structures (like Bloom filters, count-min sketch, HyperLogLog) to approximate event frequencies or membership in sets.\n",
      "    *   **Example:** Count the approximate number of unique IP addresses that have accessed a web server without storing the entire list of IPs.\n",
      "    *   **Use Case:** Network monitoring, anomaly detection, data mining.\n",
      "    *   **Trade-off:** Loss of accuracy; only provides approximate results.\n",
      "\n",
      "**Considerations for Implementing Event Compaction:**\n",
      "\n",
      "*   **Application-Specific Requirements:** The best event compaction technique will depend on the specific application and the types of information that are most important to retain.\n",
      "\n",
      "*   **Trade-offs:** There is always a trade-off between memory usage and information loss. It's important to carefully consider the implications of each technique and choose the one that best balances these factors.\n",
      "\n",
      "*   **Dynamic Adjustment:**  The compaction strategy might need to be adjusted dynamically based on the current state of the environment or the agent's learning goals. For example, an agent might choose to store more detailed event data when it is learning a new task, and then switch to a more aggressive compaction strategy once it has mastered the task.\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "Event compaction is an essential technique for managing memory usage in AI agents by reducing the amount of historical event data that needs to be stored. It involves summarizing, filtering, or aggregating events to create a more compact representation of past activity, while preserving essential information for analysis, learning, or decision-making. The choice of event compaction technique depends on the specific application and the trade-off between memory usage and information loss.\n",
      "\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "User: Why is summarization needed?\n",
      "\n",
      "Agent:\n",
      "Summarization is needed in the context of event compaction for AI agents because it addresses the challenge of managing and utilizing vast amounts of historical event data within the constraints of limited memory and computational resources. Here's a breakdown of why it's important:\n",
      "\n",
      "*   **Reducing Memory Footprint:** AI agents, especially those designed for long-term learning and decision-making, generate and encounter a massive stream of data representing past events. Storing every single event in its raw form quickly becomes impractical due to memory limitations. Summarization techniques compress this data by extracting the most salient information and discarding redundant or less important details, thereby significantly reducing the memory footprint.\n",
      "\n",
      "*   **Enabling Efficient Retrieval:** When agents need to access historical data for reasoning, planning, or learning, searching through a massive, unorganized event log would be incredibly slow and inefficient. Summarization structures data to enable quicker retrieval of relevant information. For instance, instead of scanning thousands of individual events, an agent might only need to consult a concise summary of key trends or patterns.\n",
      "\n",
      "*   **Facilitating Long-Term Learning:** Long-term learning algorithms often require access to historical data to identify patterns, refine strategies, and adapt to changing environments. Summarization allows these algorithms to work with a manageable amount of data, enabling them to learn effectively without being overwhelmed by the sheer volume of information. Techniques like temporal abstraction, which summarize events over different time scales, are especially useful for identifying long-term trends.\n",
      "\n",
      "*   **Improving Decision-Making:** By distilling historical data into meaningful summaries, agents can make more informed decisions. Summarization can help agents identify frequently occurring situations, anticipate potential risks, or recognize opportunities based on past experiences. For example, a summary of past sensor readings might reveal a pattern that predicts an impending equipment failure, allowing the agent to take proactive measures.\n",
      "\n",
      "*   **Overcoming Computational Constraints:** Processing large datasets requires significant computational power. Summarization reduces the computational burden by allowing agents to work with smaller, more manageable representations of historical data. This is particularly important for agents deployed on resource-constrained devices or in real-time environments where quick processing is essential.\n",
      "\n",
      "In essence, summarization acts as a crucial enabler for AI agents that need to learn from the past, plan for the future, and make intelligent decisions in dynamic environments. It provides a way to condense vast amounts of information into a compact and accessible form, allowing agents to overcome memory limitations, improve processing efficiency, and extract meaningful insights from historical data.\n",
      "\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "User: Explain transformers briefly.\n",
      "\n",
      "Agent:\n",
      "Okay, here's a brief explanation of Transformers, focusing on the key aspects:\n",
      "\n",
      "**What they are:**\n",
      "\n",
      "*   Transformers are a type of neural network architecture primarily designed for sequence-to-sequence tasks, like machine translation, text summarization, and text generation. They've become incredibly influential and form the basis for many state-of-the-art language models.\n",
      "\n",
      "**Key Innovations (vs. Recurrent Neural Networks like LSTMs):**\n",
      "\n",
      "*   **Attention Mechanism:** This is the heart of the Transformer. Instead of processing sequential data step-by-step, Transformers use attention to weigh the importance of different parts of the input sequence when processing each element. This allows the model to capture long-range dependencies much more effectively than recurrent networks.\n",
      "\n",
      "*   **Parallelization:** Because the attention mechanism allows the model to consider all parts of the input simultaneously (to a degree), Transformers can process sequences in parallel.  This leads to significant speedups in training compared to RNNs, which are inherently sequential.\n",
      "\n",
      "*   **Self-Attention:**  A specific type of attention where the input sequence is attending to itself. This allows the model to understand the relationships between different words (or tokens) in the same input.\n",
      "\n",
      "**Core Components:**\n",
      "\n",
      "*   **Encoder:** Processes the input sequence and creates a representation of it.  It consists of multiple layers, each containing self-attention and feed-forward neural networks.\n",
      "\n",
      "*   **Decoder:** Generates the output sequence, using the encoder's representation and attending to previously generated tokens. Like the encoder, it's composed of multiple layers with self-attention, attention to the encoder output, and feed-forward networks.\n",
      "\n",
      "*   **Embeddings:** Convert words (or tokens) into numerical vectors that the model can process.\n",
      "\n",
      "*   **Positional Encoding:** Because Transformers don't inherently process sequential data in order, positional encodings are added to the input embeddings to provide information about the position of each element in the sequence.\n",
      "\n",
      "**How it works (Simplified):**\n",
      "\n",
      "1.  **Input Embedding and Positional Encoding:** Input text is converted into numerical embeddings, and positional information is added.\n",
      "2.  **Encoder:** The encoder processes the embedded input through multiple layers of self-attention and feed-forward networks, creating a contextualized representation.\n",
      "3.  **Decoder:** The decoder takes the encoder's output and, step-by-step, generates the output sequence. It uses self-attention to focus on previously generated tokens and attention to the encoder output to relate the output to the input.\n",
      "4.  **Output:** The decoder produces a probability distribution over the vocabulary, and the most likely word (or token) is selected as the output for that step.\n",
      "\n",
      "**In Essence:**\n",
      "\n",
      "Transformers leverage attention mechanisms to efficiently process sequential data in parallel, capturing long-range dependencies and achieving state-of-the-art results in various natural language processing tasks.\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "\n",
    "from google.adk import Agent\n",
    "from google.adk.apps.app import App, EventsCompactionConfig\n",
    "from google.adk.apps.llm_event_summarizer import LlmEventSummarizer\n",
    "from google.adk.models import Gemini\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.genai import types\n",
    "\n",
    "from config import config\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Configuration\n",
    "# -------------------------------------------------\n",
    "os.environ[\"GOOGLE_API_KEY\"] = config.GOOGLE_API_KEY\n",
    "\n",
    "APP_NAME = \"my_agent_app\"   # must be valid identifier\n",
    "USER_ID = \"user_1\"\n",
    "SESSION_ID = \"session_1\"\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1️⃣ Root Agent (Main Conversational Model)\n",
    "# -------------------------------------------------\n",
    "root_agent = Agent(\n",
    "    name=\"MainAgent\",\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    instruction=\"\"\"\n",
    "You are a helpful AI assistant.\n",
    "Provide clear and structured explanations.\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2️⃣ Summarization Model (Used for Compaction)\n",
    "# -------------------------------------------------\n",
    "summarization_llm = Gemini(\n",
    "    model=\"gemini-2.5-flash\"   # Used ONLY for summarization\n",
    ")\n",
    "\n",
    "my_summarizer = LlmEventSummarizer(\n",
    "    llm=summarization_llm\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3️⃣ Create App with Events Compaction Config\n",
    "# -------------------------------------------------\n",
    "app = App(\n",
    "    name=APP_NAME,\n",
    "    root_agent=root_agent,\n",
    "    events_compaction_config=EventsCompactionConfig(\n",
    "        compaction_interval=3,  # Compact after every 3 events\n",
    "        overlap_size=1,         # Keep last 1 event before summarizing\n",
    "        summarizer=my_summarizer,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4️⃣ Run the App\n",
    "# -------------------------------------------------\n",
    "session_service = InMemorySessionService()\n",
    "\n",
    "# Create session\n",
    "await session_service.create_session(\n",
    "    app_name=APP_NAME,\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_ID,\n",
    ")\n",
    "\n",
    "runner = Runner(\n",
    "    app=app,   # Important: pass App, not Agent\n",
    "    session_service=session_service,\n",
    ")\n",
    "\n",
    "print(\"Starting Conversation...\\n\")\n",
    "\n",
    "user_messages = [\n",
    "    \"Explain what AI agents are.\",\n",
    "    \"How do they use memory?\",\n",
    "    \"What is event compaction?\",\n",
    "    \"Why is summarization needed?\",\n",
    "    \"Explain transformers briefly.\"\n",
    "]\n",
    "\n",
    "for msg in user_messages:\n",
    "    print(\"=\"*100)\n",
    "    print(f\"User: {msg}\\n\")\n",
    "\n",
    "    async for event in runner.run_async(\n",
    "        user_id=USER_ID,\n",
    "        session_id=SESSION_ID,\n",
    "        new_message=types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[types.Part(text=msg)],\n",
    "        ),\n",
    "    ):\n",
    "        if event.content and event.content.parts:\n",
    "            print(\"Agent:\")\n",
    "            print(event.content.parts[0].text)\n",
    "            print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccb3a14-8200-4953-82eb-dd6801972a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
